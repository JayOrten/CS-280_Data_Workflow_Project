# -*- coding: utf-8 -*-
"""Tool Lab 3 Tensorflow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_YmPTRNsl9wt0k0LjjZv2W_DS7xnL7w5
"""

!pip install wandb

import numpy as np
import tensorflow as tf
from tensorflow import keras
from keras import layers
import wandb
from wandb.keras import WandbMetricsLogger
import matplotlib.pyplot as plt
import random

# get your data
(x_train, y_train), (x_val, y_val) = keras.datasets.cifar100.load_data()

# onehot encode your labels so your model knows its a category
y_train = tf.one_hot(y_train,
                     depth=y_train.max() + 1,
                     dtype=tf.float64)
y_val = tf.one_hot(y_val,
                   depth=y_val.max() + 1,
                   dtype=tf.float64)
  
y_train = tf.squeeze(y_train)
y_val = tf.squeeze(y_val)

checkpointCallbackFunction =  tf.keras.callbacks.ModelCheckpoint(
                                filepath='/checkpoints/model.{epoch:02d}-{val_loss:.2f}.h5',
                                save_freq="epoch")

# constant values
NUM_CLASSES = 100 #100 prediction classes
INPUT_SHAPE = (32,32,3) #shape of the input image 32x32 with 3 channels

# hyperparameters you will be tuning
BATCH_SIZE = 40
EPOCHS = 10
LEARNING_RATE = 0.0001
L1NF = 256
L2NF = 512
L3NF = 512
FDROPOUT = 0.6

wandb.init(project = 'Tool Lab 3',
          config={
              'bs':BATCH_SIZE,
              'lr':LEARNING_RATE,
              'epochs': EPOCHS,
              'l1nf':L1NF,
              'l2nf':L2NF,
              'l3nf':L3NF,
              'fdr':FDROPOUT
          },
           name='RUN 19: NUM_LAYERS: 3 ~ BATCH_SIZE: {} ~ EPOCHS: {} ~ LR: {} ~ L1NF: {} ~ L2NF: {} ~ L3NF+: {} ~ FDROPOUT: {} -- Try different batch size'.format(BATCH_SIZE, EPOCHS, LEARNING_RATE, L1NF, L2NF, L3NF, FDROPOUT) #this is your run name, please number your runs 0-n and tell what you changed
          )

# here is a basic model that you will add to
model = tf.keras.models.Sequential([
                  
                  # CHANGE THESE: these are layers you should mix up and change
                  layers.Conv2D(L1NF, (3, 3), input_shape = INPUT_SHAPE,
                                activation='relu',
                                padding='same'),
                  layers.Conv2D(L1NF, (3, 3),
                                activation='relu',
                                padding='same'),
                  layers.Conv2D(L1NF, (3, 3),
                                activation='relu',
                                padding='same'),
                  layers.MaxPooling2D(2,2),

                  layers.Conv2D(L2NF, (3, 3),
                                activation='relu',
                                padding='same'),
                  layers.Conv2D(L2NF, (3, 3),
                                activation='relu',
                                padding='same'),
                  layers.Conv2D(L2NF, (3, 3),
                                activation='relu',
                                padding='same'),
                  layers.MaxPooling2D(2,2),

                  layers.Conv2D(L3NF, (3, 3),
                                activation='relu',
                                padding='same'),
                  layers.Conv2D(L3NF, (3, 3),
                                activation='relu',
                                padding='same'),
                  layers.Conv2D(L3NF, (3, 3),
                                activation='relu',
                                padding='same'),
                  layers.MaxPooling2D(2,2),
                  layers.Dropout(FDROPOUT),
                  
                  # DO NOT CHANGE THESE. They should be at the end of your model
                  layers.Flatten(),
                  layers.Dense(NUM_CLASSES, activation='softmax')])

# feel free to experiment with this
model.compile(loss='categorical_crossentropy',
             optimizer=keras.optimizers.RMSprop(learning_rate=LEARNING_RATE),
             # DO NOT CHANGE THE METRIC. This is what you will be judging your model on
             metrics=['accuracy'],)

# here you train the model using some of your hyperparameters and send the data
# to weights and biases after every batch            
history = model.fit(x_train, y_train,
                    epochs=EPOCHS,
                    batch_size=BATCH_SIZE,
                    verbose=1,
                    validation_data=(x_val, y_val),
                    callbacks=[WandbMetricsLogger(log_freq='batch'),
                               checkpointCallbackFunction])

# Save
model.save("models/trainedModel.h5")

#This will load my model
model = keras.models.load_model("models/trainedMode.h5")